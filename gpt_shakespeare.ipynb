{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/homerkay1/colab_repo/blob/main/gpt_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken\n",
        "\n",
        "# https://kean-chan.medium.com/creating-and-exploring-gpt-from-scratch-ffe84ac415a9\n",
        "# This is the article used here."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "S3fYHG5aB3Cg",
        "outputId": "273eb4f8-9dcb-433e-f671-d7aaad2de16c"
      },
      "id": "S3fYHG5aB3Cg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5a719719e91a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install tiktoken\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7923f51",
      "metadata": {
        "id": "d7923f51"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f007e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52f007e6",
        "outputId": "3e1d5ed9-c688-4955-bea4-5eab4a600d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "data_dir = os.path.join('data', 'tinyshakespeare')\n",
        "input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    os.makedirs(data_dir)\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_dir, 'val.bin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b675faf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b675faf",
        "outputId": "aed96618-dbfd-409e-f67b-076a79e4db10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "type(train_data)\n",
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de679a27",
      "metadata": {
        "id": "de679a27"
      },
      "outputs": [],
      "source": [
        " # Model Configuration\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "class CustomConfig(GPTConfig):\n",
        "    n_layer = 8\n",
        "    n_head = 8\n",
        "    n_embd = 256\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "    dropout = 0.1\n",
        "    compile = True\n",
        "    device = 'cuda'\n",
        "    num_workers = 0\n",
        "    max_iters = 2e4\n",
        "    batch_size = 4\n",
        "    block_size = 64\n",
        "    learning_rate = 6e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    weight_decay = 1e-1\n",
        "    grad_norm_clip = 1.0\n",
        "\n",
        "vocab_size = len(train_ids)\n",
        "config = CustomConfig(vocab_size=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad38af2c",
      "metadata": {
        "id": "ad38af2c"
      },
      "outputs": [],
      "source": [
        "# Datasets and Data Loaders\n",
        "# read data from .bin\n",
        "data_dir = os.path.join('data', 'tinyshakespeare')\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, split, block_size=128, device_type='cuda'):\n",
        "        assert split in {'train', 'test'}\n",
        "        self.split = split\n",
        "        self.block_size = block_size\n",
        "        self.device_type = device_type\n",
        "        self.data = train_data if split == 'train' else val_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n",
        "        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64))\n",
        "\n",
        "        if self.device_type == 'cuda':\n",
        "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "            x, y = x.pin_memory().to('cuda', non_blocking=True), y.pin_memory().to('cuda', non_blocking=True)\n",
        "        else:\n",
        "            x, y = x.to('cpu'), y.to('cpu')\n",
        "        return x, y\n",
        "\n",
        "# create dataset and dataloader\n",
        "train_dataset = ShakespeareDataset('train', config.block_size, config.device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
        "test_dataset = ShakespeareDataset('test', config.block_size, config.device)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004eb6d3",
      "metadata": {
        "id": "004eb6d3"
      },
      "outputs": [],
      "source": [
        "# Model Implementation\n",
        "# GELU Activation Func\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2543d9c9",
      "metadata": {
        "id": "2543d9c9"
      },
      "outputs": [],
      "source": [
        "# Causal self Attention, restricts attention mechanism to\n",
        "# look only at the previous tokens in the sequence\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection\n",
        "    at the end.\n",
        "    It's important in decoder block to have diagonal mask\n",
        "    It is also possible to use torch.nn.MultiheadAttention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.dropout = config.dropout\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(\n",
        "                        torch.nn.functional,\n",
        "                        'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\n",
        "              \"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size)\n",
        "            ).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, seq_len, emb_dim\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            # diagonal mask\n",
        "            # fill 0 mask with super small number so it wont affect the softmax weight\n",
        "            # (batch_size, h, seq_len, seq_len)\n",
        "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "\n",
        "            # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n",
        "            y = att @ v\n",
        "\n",
        "        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260168ac",
      "metadata": {
        "id": "260168ac"
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "class Block(nn.Module):\n",
        "    \"\"\" GPT decoder block\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # (batch_size, seq_len, emb_dim)\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5495e062",
      "metadata": {
        "id": "5495e062"
      },
      "outputs": [],
      "source": [
        "# The GPT\n",
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                # random note: because named_modules and named_parameters are recursive\n",
        "                # we will see the same tensors p many many times. but doing it this way\n",
        "                # allows us to know which parent module any tensor p belongs to...\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "\n",
        "        # positional token, shape (1, t)\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        # (b, t, n_embd) -- > # (b, t, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        # -1 at output will be ignored\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62226932",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62226932",
        "outputId": "17bea2c2-a9c1-417d-f030-cf3820dad412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 83.64M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n",
            "[2023-04-24 02:17:19,272] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 12.67436\n",
            "iter_dt 26.62ms; iter 500: train loss 5.01137\n",
            "iter_dt 26.63ms; iter 1000: train loss 4.93758\n",
            "iter_dt 26.53ms; iter 1500: train loss 4.68756\n",
            "iter_dt 26.50ms; iter 2000: train loss 5.21746\n",
            "iter_dt 26.45ms; iter 2500: train loss 4.77478\n",
            "iter_dt 26.69ms; iter 3000: train loss 3.89031\n",
            "iter_dt 26.52ms; iter 3500: train loss 4.91066\n",
            "iter_dt 26.64ms; iter 4000: train loss 4.69797\n",
            "iter_dt 26.37ms; iter 4500: train loss 4.22820\n",
            "iter_dt 26.52ms; iter 5000: train loss 3.92760\n",
            "iter_dt 26.48ms; iter 5500: train loss 4.05628\n",
            "iter_dt 26.51ms; iter 6000: train loss 3.78237\n",
            "iter_dt 26.56ms; iter 6500: train loss 4.65464\n",
            "iter_dt 26.51ms; iter 7000: train loss 4.28562\n",
            "iter_dt 25.80ms; iter 7500: train loss 4.05964\n",
            "iter_dt 27.19ms; iter 8000: train loss 4.01703\n",
            "iter_dt 35.68ms; iter 8500: train loss 3.38909\n",
            "iter_dt 26.45ms; iter 9000: train loss 3.95278\n",
            "iter_dt 26.62ms; iter 9500: train loss 4.17324\n",
            "iter_dt 26.43ms; iter 10000: train loss 4.21105\n",
            "iter_dt 25.82ms; iter 10500: train loss 3.89776\n",
            "iter_dt 27.08ms; iter 11000: train loss 3.92969\n",
            "iter_dt 26.60ms; iter 11500: train loss 4.35181\n",
            "iter_dt 26.40ms; iter 12000: train loss 3.86603\n",
            "iter_dt 26.70ms; iter 12500: train loss 4.00511\n",
            "iter_dt 26.53ms; iter 13000: train loss 3.80189\n",
            "iter_dt 26.49ms; iter 13500: train loss 3.26467\n",
            "iter_dt 26.17ms; iter 14000: train loss 4.15475\n",
            "iter_dt 25.89ms; iter 14500: train loss 3.95488\n",
            "iter_dt 26.65ms; iter 15000: train loss 3.85053\n",
            "iter_dt 26.81ms; iter 15500: train loss 3.56763\n",
            "iter_dt 26.63ms; iter 16000: train loss 3.14201\n",
            "iter_dt 26.60ms; iter 16500: train loss 4.75306\n",
            "iter_dt 26.40ms; iter 17000: train loss 3.78798\n",
            "iter_dt 26.77ms; iter 17500: train loss 4.29025\n",
            "iter_dt 26.45ms; iter 18000: train loss 4.17214\n",
            "iter_dt 26.39ms; iter 18500: train loss 3.17981\n",
            "iter_dt 26.62ms; iter 19000: train loss 3.63521\n",
            "iter_dt 26.59ms; iter 19500: train loss 3.17490\n"
          ]
        }
      ],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device = config.device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # variables that will be assigned to trainer class later for logging and etc\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "\n",
        "        # setup the optimizer\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "\n",
        "        # setup the dataloader\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
        "            shuffle=False,\n",
        "            # pin_memory=True,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=config.num_workers,\n",
        "        )\n",
        "\n",
        "        model.train()\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = time.time()\n",
        "        data_iter = iter(train_loader)\n",
        "        while True:\n",
        "\n",
        "            # fetch the next batch (x, y) and re-init iterator if needed\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                batch = next(data_iter)\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            x, y = batch\n",
        "\n",
        "            # forward the model\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.trigger_callbacks('on_batch_end')\n",
        "            self.iter_num += 1\n",
        "            tnow = time.time()\n",
        "            self.iter_dt = tnow - self.iter_time\n",
        "            self.iter_time = tnow\n",
        "\n",
        "            # termination conditions\n",
        "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
        "                break\n",
        "\n",
        "model = GPT(config).to(config.device)\n",
        "if config.compile:\n",
        "     model = torch.compile(model)\n",
        "trainer = Trainer(config, model, train_dataset)\n",
        "trainer = Trainer(config, model, train_dataset)\n",
        "\n",
        "def batch_end_callback(trainer):\n",
        "    if trainer.iter_num % 500 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688ca002",
      "metadata": {
        "id": "688ca002",
        "outputId": "0a3b40ef-f42d-4d56-c200-3ca35e5a45b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lord:\n",
            "Rise! My people, conquer the north!\n",
            "Thy father, and thy mother, and thy son,\n",
            "And, by thy father, by thy father's love,\n",
            "And, by thy father, and thou art dead.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "LAD\n"
          ]
        }
      ],
      "source": [
        "text = 'Lord:\\nRise! My people, conquer the north!'\n",
        "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
        "sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
        "result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
        "print(enc.decode(result.detach().cpu().tolist()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Lord:\\nJennifer, oh where art thou'\n",
        "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
        "sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
        "result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
        "print(enc.decode(result.detach().cpu().tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxytTB-DF0PK",
        "outputId": "9bd50848-d63e-4a2c-d92e-8c09cc02a6ce"
      },
      "id": "WxytTB-DF0PK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lord:\n",
            "Jennifer, oh where art thou?\n",
            "\n",
            "SICINIUS:\n",
            "I know the very well:\n",
            "And, to the Capitol, where you are\n",
            "To do the Capitol, where you shall\n",
            "To the Capitol.\n",
            "\n",
            "BRUTUS:\n",
            "I have a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7407349",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7407349",
        "outputId": "12941f2e-1ea5-416e-ff43-5bff48964942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 24 02:29:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    53W / 400W |   4197MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        " #What GPU Am I On?\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL SAVED, NO NEED TO RUN\n",
        "\n",
        "model_dir = os.path.join('model', 'model_a')\n",
        "input_file_path_2 = os.path.join(model_dir, 'model_shakes')\n",
        "if not os.path.exists(input_file_path):\n",
        "    os.makedirs(model_dir)\n",
        "torch.save(model.state_dict(), input_file_path_2)"
      ],
      "metadata": {
        "id": "6qu9GjFLCToj"
      },
      "id": "6qu9GjFLCToj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD MODEL:\n",
        "\n",
        "model_load = GPT(config).to(config.device)\n",
        "if config.compile:\n",
        "     model_load = torch.compile(model_load)\n",
        "model_load.load_state_dict(torch.load(input_file_path_2))\n",
        "model_load.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Iyr5X5RGozf",
        "outputId": "35ef0ead-fa02-4445-ac5b-1b31818ecb59"
      },
      "id": "1Iyr5X5RGozf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 83.64M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): GPT(\n",
              "    (transformer): ModuleDict(\n",
              "      (wte): Embedding(301966, 256)\n",
              "      (wpe): Embedding(64, 256)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-7): 8 x Block(\n",
              "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): CausalSelfAttention(\n",
              "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
              "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): ModuleDict(\n",
              "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (act): NewGELU()\n",
              "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=256, out_features=301966, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## INFERENCE ON LOADED MODEL\n",
        "text = 'Lord Hayden:\\nI hath boughteth mah boys the Bose Soundliketh Flexeth'\n",
        "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
        "sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
        "result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
        "print(enc.decode(result.detach().cpu().tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z0E6x-DJAxi",
        "outputId": "bdd3abe2-ddb7-4db9-f68e-7a78a63b4989"
      },
      "id": "2Z0E6x-DJAxi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lord Hayden:\n",
            "I hath boughteth mah boys the Bose Soundliketh Flexeth\n",
            "And in his highness' bosom:\n",
            "And, if you be a king, and I\n",
            "With all his noble gentleman,\n",
            "And I, that he shall not stay with him,\n",
            "And that the king's will be his daughter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdfGyu68JGyt"
      },
      "id": "FdfGyu68JGyt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}